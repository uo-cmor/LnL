---
title: "Introduction to Bayesian Analysis"
subtitle: "CMOR Lunch'n'Learn"
author: "Ross Wilson"
date: "2023-11-23"
date-format: "D MMMM YYYY"
format:
  revealjs:
    theme: [default, cmor.scss]
    title-slide-attributes: 
      data-background-image: "images/otago-pennant.png, images/cmor-banner.png, images/title-background.png"
      data-background-size: "7%, 25%, 100%"
      data-background-position: "3% 1%, bottom 1% left 50px,0 0"
      data-background-repeat: "no-repeat"
    template-partials:
      - styles.html
      - title-slide.html
    margin: 0.2
    footer: "[Return to CMOR Lunch'n'Learn presentations](../index.html)"
---
```{r setup}
library(tidyverse)
library(kableExtra, exclude = "group_rows")
```

# What is Bayesian analysis?

## Probability-based analysis

<br>

* Bayesian analysis is about estimating/interpreting the *uncertainty* or *probability distribution* of parameters

<br>

* To understand this, we need a very brief background to probability theory

## What is probability?

. . .

<br>

Or, how should we interpret a probability of, say, 20%?

<br>

:::: {.columns}

::: {.column .fragment width="45%"}
**Relative frequency**

<br>

$\mathrm{Pr}(A) = \lim\limits_{n \to \infty} \frac{\text{# times } A \text{ occurs}}{n}$

<br>

Classical statistics is based on this concept of repeated sampling

:::

::: {.column width="10%"}

:::

::: {.column .fragment width="45%"}
**Subjective probability**

*A measure of certainty (or uncertainty) in an event*

::: {.fragment}
* What is the probability it will rain in Dunedin tomorrow?

* What is the probability I had chicken for dinner last night?
:::
:::

::::

## Conditional probability

<br>

* Probability can be defined conditional on information

::: {.incremental}

* What is the probability it will rain in Dunedin tomorrow?

  * $\mathrm{Pr}(\text{Rain tomorrow})$

  * $\mathrm{Pr}(\text{Rain tomorrow}\ |\ \text{Today's weather})$

  * $\mathrm{Pr}(\text{Rain tomorrow}\ |\ \text{Tomorrow's forecast})$
  
:::

## What is Bayesian analysis?

<br>

* Bayesian analysis is the process of updating these probabilities in light of new information

<br>

* In a statistical model, we are usually interested in probability distributions over continuous covariates:

$$f(\theta|y)$$

(updated knowledge about a parameter $\theta$ given data $y$)

## Bayes' Theorem

<br>

<br>

$$\mathrm{Pr}(A | B) = \frac{\mathrm{Pr}(B | A) \mathrm{Pr}(A)}{\mathrm{Pr}(B)}$$

## Bayes' Theorem

* This tells us how to calculate the 'posterior distribution' given:
  * an assumed data generating model
  * prior information/beliefs about the parameter(s)
  * current data

. . .

$$f(\theta | y) = \frac{f(y | \theta) f(\theta)}{f(y)}$$

. . .

$$\mathrm{Posterior} \propto \mathrm{Likelihood} \times \mathrm{Prior}$$

. . .

* Informally: updated belief = current evidence $\times$ prior evidence or belief

## Bayes' Theorem

* Conceptually,

  * we start with some beliefs about the parameters of interest
  
  * review those beliefs in light of the evidence at hand
  
  * and calculate an updated belief as a combination of the prior and the new evidence

![](images/density-1.png){.absolute bottom=-60 left=230 width=675 .fragment fragment-index=1}

![](images/density-2.png){.absolute bottom=-60 left=230 width=675 .fragment fragment-index=2}

![](images/density-3.png){.absolute bottom=-60 left=230 width=675 .fragment fragment-index=3}

::: {.absolute bottom=180 left=430 .fragment fragment-index=1}
[Prior]{style="color:#9999FF;"}
:::


::: {.absolute bottom=130 left=750 .fragment fragment-index=2}
[Likelihood]{style="color:#FF9999;"}
:::


::: {.absolute bottom=300 left=650 .fragment fragment-index=3}
[Posterior]{style="color:#C76F8E;"}
:::
## Bayes' Theorem

![](images/more-data-1.png){.absolute bottom=-60 left=410 width=600 .fragment fragment-index=1}

![](images/more-data-2.png){.absolute bottom=-60 left=410 width=600 .fragment fragment-index=2}

![](images/more-data-3.png){.absolute bottom=-60 left=410 width=600 .fragment fragment-index=3}

* Where do these 'priors' come from?

  * Previous research
  
  * Common sense/intuition?
  
  * 'Weakly informative' priors

* The more data we have, the less the prior matters

## Numeric example
* Consider a simple repeated coin toss:
  * Say we toss a coin 10 times, and it comes up heads 7/10
* Maximum likelihood analysis:

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> <b>Hypothesis</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(\mathit{heads}) = \theta\)</span> </th>
   <th style="text-align:center;"> <b>Prior</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(\theta)\)</span> </th>
   <th style="text-align:center;"> <b>Likelihood</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(y \mid \theta)\)</span> </th>
   <th style="text-align:center;"> <b>Posterior</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(\theta \mid y)\)</span> </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 0.2 </td>
   <td style="text-align:center;"> 0.01 </td>
   <td style="text-align:center;"> 0.0008 </td>
   <td style="text-align:center;"> 0.0001 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.3 </td>
   <td style="text-align:center;"> 0.04 </td>
   <td style="text-align:center;"> 0.0090 </td>
   <td style="text-align:center;"> 0.0030 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.4 </td>
   <td style="text-align:center;"> 0.10 </td>
   <td style="text-align:center;"> 0.0425 </td>
   <td style="text-align:center;"> 0.0351 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.5 </td>
   <td style="text-align:center;"> 0.70 </td>
   <td style="text-align:center;"> 0.1172 </td>
   <td style="text-align:center;"> 0.6789 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.6 </td>
   <td style="text-align:center;"> 0.10 </td>
   <td style="text-align:center;"> 0.2150 </td>
   <td style="text-align:center;"> 0.1779 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.7 </td>
   <td style="text-align:center;"> 0.04 </td>
   <td style="text-align:center;"> 0.2668 </td>
   <td style="text-align:center;"> 0.0883 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.8 </td>
   <td style="text-align:center;"> 0.01 </td>
   <td style="text-align:center;"> 0.2013 </td>
   <td style="text-align:center;"> 0.0167 </td>
  </tr>
</tbody>
</table>
```

## References

* Recommended textbooks:
  * Introductory: *Doing Bayesian Data Analysis: A tutorial with R, JAGS, and Stan*, 2^nd^ Edition. John K. Kruschke (2015)
    * Mostly non-technical presentation of applied Bayesian analysis
    * e-Book available for download via the library
  * Intermediate: *Statistical Rethinking: A Bayesian Course with Examples in R and Stan*, 2^nd^ Edition. Richard McElreath (2020)
    * How to think about statistics in terms of data generating models
    * I couldn't find an e-Book version, but there are video lectures by the author on YouTube: <https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus>
  * Advanced: *Bayesian Data Analysis*, 3^rd^ Edition. Andrew Gelman, John B. Carlin, Hal S. Stearn, David B. Dunson, Aki Vehtari, and Donald B. Rubin (2013)
    * Parts of this are quite advanced, but it also has lots of useful examples and applications
    * PDF version available from Gelman's website: <http://www.stat.columbia.edu/~gelman/book/>

## References

* Other useful resources:
  * Much of the background theory here follows *Bayesian Basics*, Michael Clark (2018): <https://m-clark.github.io/bayesian-basics/>
  * Stan and `rstan` both have extensive documentation, examples, etc. -- See <https://mc-stan.org>
  * Other R packages (`rstanarm`, `brms`, `edstan`, etc.) have their own documentation and usually good vignettes, tutorials, and/or examples
