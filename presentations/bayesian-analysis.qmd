---
title: "Introduction to Bayesian Analysis"
subtitle: "CMOR Lunch'n'Learn"
author: "Ross Wilson"
date: "2023-11-23"
date-format: "D MMMM YYYY"
format:
  revealjs:
    theme: [default, cmor.scss]
    title-slide-attributes: 
      data-background-image: "images/otago-pennant.png, images/cmor-banner.png, images/title-background.png"
      data-background-size: "7%, 25%, 100%"
      data-background-position: "3% 1%, bottom 1% left 50px,0 0"
      data-background-repeat: "no-repeat"
    template-partials:
      - styles.html
      - title-slide.html
    margin: 0.2
    footer: "[Return to CMOR Lunch'n'Learn presentations](../index.html)"
---
```{r setup}
library(tidyverse)
library(kableExtra, exclude = "group_rows")
```

# What is Bayesian analysis?

## Probability-based analysis

<br>

* Bayesian analysis is about estimating/interpreting the *uncertainty* or *probability distribution* of parameters

<br>

* To understand this, we need a very brief background to probability theory

## What is probability?

<br>

Or, how should we interpret a probability of, say, 20%?

<br>

::: {.columns}

:::: {.column .fragment width="45%"}
**Relative frequency**

<br>

$\mathrm{Pr}(A) = \lim\limits_{n \to \infty} \frac{\text{# times } A \text{ occurs}}{n}$

<br>

::::: {.fragment}

Classical statistics is based on this concept of repeated sampling

:::::

::::

:::: {.column width="10%"}

::::

:::: {.column .fragment width="45%"}
**Subjective probability**

*A measure of certainty (or uncertainty) in an event*

::::: {.fragment}

* What is the probability it will rain in Dunedin tomorrow?

* What is the probability that exercise therapy is beneficial for a patient with OA?

:::::

::::

:::

## Conditional probability

<br>

* Probability can be defined conditional on information

::: {.incremental}

* What is the probability it will rain in Dunedin tomorrow?

  * $\mathrm{Pr}(\text{Rain tomorrow})$

  * $\mathrm{Pr}(\text{Rain tomorrow}\ |\ \text{Today's weather})$

  * $\mathrm{Pr}(\text{Rain tomorrow}\ |\ \text{Tomorrow's forecast})$
  
:::

## What is Bayesian analysis?

<br>

* Bayesian analysis is the process of updating these probabilities in light of new information

. . .

<br>

#### Bayes' Theorem:

$$\mathrm{Pr}(A | B) = \frac{\mathrm{Pr}(B | A) \mathrm{Pr}(A)}{\mathrm{Pr}(B)}$$

  * In a statistical model, we are usually interested in probability distributions over continuous covariates:

$$f(\theta | y) = \frac{f(y | \theta) f(\theta)}{f(y)}$$

  (updated knowledge about a parameter $\theta$ given data $y$)

## Bayes' Theorem

* Tells us how to calculate the 'posterior distribution' given:
  * an assumed data generating model
  * prior information/beliefs about the parameter(s)
  * current data

. . .

$$f(\theta | y) = \frac{f(y | \theta) f(\theta)}{f(y)}$$

. . .

$$\mathrm{Posterior} \propto \mathrm{Likelihood} \times \mathrm{Prior}$$

. . .

* Informally: updated belief = current evidence $\times$ prior evidence or belief

## Bayes' Theorem

* Conceptually,

  * we start with some beliefs about the parameters of interest
  
  * review those beliefs in light of the evidence at hand
  
  * and calculate an updated belief as a combination of the prior and the new evidence
  
```{r}
density_tbl <- tibble(
  theta = seq(0, 1, length.out = 99999),
  prior = dbeta(theta, 10, 10),
  likelihood = dbeta(theta, 9, 6),
  posterior = dbeta(theta, 19, 16)
)

ymax <- max(density_tbl$prior, density_tbl$likelihood, density_tbl$posterior)
```

```{r}
p1 <- ggplot(density_tbl, aes(theta)) +
    geom_area(aes(y = prior), linetype = 0, fill = "#8888FF66", stat = "identity", position = "identity") +
    scale_x_continuous(limits = c(0, 1)) +
    scale_y_continuous("Density", limits = c(0, ymax), expand = c(0, 0)) +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.line.x = element_blank(),
          panel.background = element_blank())
p2 <- p1 +
  geom_area(aes(y = likelihood), linetype = 0, fill = "#FF888866", stat = "identity", position = "identity")
p3 <- p2 +
  geom_area(aes(y = posterior), linetype = 0, fill = "#EE88CC66", stat = "identity", position = "identity")
ggsave("images/density-1.png", p1, width = 6, height = 4)
ggsave("images/density-2.png", p2, width = 6, height = 4)
ggsave("images/density-3.png", p3, width = 6, height = 4)
```

![](images/density-1.png){.absolute bottom=-60 left=230 width=675 .fragment fragment-index=1}

![](images/density-2.png){.absolute bottom=-60 left=230 width=675 .fragment fragment-index=2}

![](images/density-3.png){.absolute bottom=-60 left=230 width=675 .fragment fragment-index=3}

::: {.absolute bottom=180 left=430 .fragment fragment-index=1}
[Prior]{style="color:#9999FF;"}
:::


::: {.absolute bottom=130 left=750 .fragment fragment-index=2}
[Likelihood]{style="color:#FF9999;"}
:::


::: {.absolute bottom=300 left=650 .fragment fragment-index=3}
[Posterior]{style="color:#C76F8E;"}
:::
## Bayes' Theorem

* Where do these 'priors' come from?

  * Previous research
  
  * Common sense/intuition?
  
  * 'Weakly informative' priors

* The more data we have, the less the prior matters

```{r}
density_tbl2 <- density_tbl |> 
  mutate(
  likelihood2 = dbeta(theta, 90, 60),
  posterior2 = dbeta(theta, 100, 70)
)

ymax2 <- max(density_tbl2$prior, density_tbl2$likelihood2, density_tbl2$posterior2)

p4 <- ggplot(density_tbl2, aes(theta)) +
    geom_area(aes(y = prior), linetype = 0, fill = "#8888FF66", stat = "identity", position = "identity") +
    scale_x_continuous(limits = c(0, 1)) +
    scale_y_continuous(NULL, limits = c(0, ymax2), expand = c(0, 0)) +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.line.x = element_blank(), panel.grid = element_blank(),
          panel.background = element_blank(), plot.background = element_blank())
p5 <- p4 +
  geom_area(aes(y = likelihood2), linetype = 0, fill = "#FF888866", stat = "identity", position = "identity")
p6 <- p5 +
  geom_area(aes(y = posterior2), linetype = 0, fill = "#EE88CC66", stat = "identity", position = "identity")
ggsave("images/more-data-1.png", p4, width = 6, height = 10)
ggsave("images/more-data-2.png", p5, width = 6, height = 10)
ggsave("images/more-data-3.png", p6, width = 6, height = 10)
```

![](images/more-data-1.png){.absolute bottom=-60 left=410 width=600 height=1000 .fragment .fade-in-then-out fragment-index=1}

![](images/more-data-2.png){.absolute bottom=-60 left=410 width=600 height=1000 .fragment .fade-in-then-out fragment-index=2}

![](images/more-data-3.png){.absolute bottom=-60 left=410 width=600 height=1000 .fragment fragment-index=3}

## Numeric example
* Consider a simple repeated coin toss:
  * Say we toss a coin 10 times, and it comes up heads 7/10
* Maximum likelihood analysis:

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> <b>Hypothesis</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(\mathit{heads}) = \theta\)</span> </th>
   <th style="text-align:center;"> <b>Prior</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(\theta)\)</span> </th>
   <th style="text-align:center;"> <b>Likelihood</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(y \mid \theta)\)</span> </th>
   <th style="text-align:center;"> <b>Posterior</b><br><span style="font-size: 0.8em;">\(\mathrm{Pr}(\theta \mid y)\)</span> </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 0.2 </td>
   <td style="text-align:center;"> 0.01 </td>
   <td style="text-align:center;"> 0.0008 </td>
   <td style="text-align:center;"> 0.0001 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.3 </td>
   <td style="text-align:center;"> 0.04 </td>
   <td style="text-align:center;"> 0.0090 </td>
   <td style="text-align:center;"> 0.0030 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.4 </td>
   <td style="text-align:center;"> 0.10 </td>
   <td style="text-align:center;"> 0.0425 </td>
   <td style="text-align:center;"> 0.0351 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.5 </td>
   <td style="text-align:center;"> 0.70 </td>
   <td style="text-align:center;"> 0.1172 </td>
   <td style="text-align:center;"> 0.6789 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.6 </td>
   <td style="text-align:center;"> 0.10 </td>
   <td style="text-align:center;"> 0.2150 </td>
   <td style="text-align:center;"> 0.1779 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.7 </td>
   <td style="text-align:center;"> 0.04 </td>
   <td style="text-align:center;"> 0.2668 </td>
   <td style="text-align:center;"> 0.0883 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 0.8 </td>
   <td style="text-align:center;"> 0.01 </td>
   <td style="text-align:center;"> 0.2013 </td>
   <td style="text-align:center;"> 0.0167 </td>
  </tr>
</tbody>
</table>
```

# Bayesian analysis in practice

## Software

* Historically, conducting Bayesian analysis required specialised software/modelling languages
  * BUGS (Bayesian inference Using Gibbs Sampling)
  * JAGS (Just Another Gibbs Sampler)
  * Stan

. . .

* Most general-purpose statistical programs these days have (at least some) Bayesian methods available
  * R
    * `rstanarm`
    * `brms`
    * direct interface to Bayesian modelling languages with `rstan`, `rjags`
  * Stata (also StataStan)
  * SPSS ?

## Example - linear regression model

* We will use the `rstanarm` package in R for illustration
* This makes it very easy to run Bayesian analogues of several standard regression models (linear, logistic, etc.)
  * (Just as easy as running the standard classical models)

. . .

* Well, almost as easy
  * There are still a few Bayesian-specific issues to consider (prior distributions, computational options, etc.)

## (Not so) technical backgroun

* Bayesian estimation is based on Markov Chain Monte Carlo (MCMC) simulation
* We won't go into the details
  * Basically, we start with some initial guesses for the parameters, then keep adjusting these until we get to a stable distribution
  * Once we get to this stable range, the distribution of parameter 'guesses' is our posterior distribution
  * Checking how well this convergence has worked is a key part of model diagnostics

## Example - linear regression model

* Linear regression models in R (standard `lm()`, Bayesian analysis with `stan` and `stan_glm()`)
* Convergence diagnostics
* More substantive model checks -- i.e., does the model make sense?
  * Sensitivity to alternative priors
    * If the results are highly sensitive to different plausible priors, that should give us less confidence in our conclusions
  * Predictive accuracy
    * How well does the model predict the actual (distribution of) data?
* Model interpretation/analysis/reporting

## Further examples

* Other common regression models are easily replicated with `rstanarm` (or `brms`)
  * Logistic regression (binary outcomes)
  * Poisson regression (count outcomes)
* These packages can also do some more complicated models
  * Multinomial or ordinal outcomes
  * Mixed/random effects
* For more complex models, you will need to use Stan (or BUGS/JAGS) directly
  * `rstan` can run these models from within R
  * `brms` can be used to generate starter code, if the model is a variant of a `brms` model

## Further examples

* 8 Schools
  * Hierarchical Bayesian modelling

. . .

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> <b>School</b> </th>
   <th style="text-align:center;"> <b>Mean estimate</b> </th>
   <th style="text-align:center;"> <b>Standard error</b> </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> A </td>
   <td style="text-align:center;"> 28 </td>
   <td style="text-align:center;"> 15 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> B </td>
   <td style="text-align:center;"> 8 </td>
   <td style="text-align:center;"> 10 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> C </td>
   <td style="text-align:center;"> -3 </td>
   <td style="text-align:center;"> 16 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> D </td>
   <td style="text-align:center;"> 7 </td>
   <td style="text-align:center;"> 11 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> E </td>
   <td style="text-align:center;"> -1 </td>
   <td style="text-align:center;"> 9 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> F </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 11 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> G </td>
   <td style="text-align:center;"> 18 </td>
   <td style="text-align:center;"> 10 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> H </td>
   <td style="text-align:center;"> 12 </td>
   <td style="text-align:center;"> 18 </td>
  </tr>
</tbody>
</table>
```

. . .

::: {.absolute right=-50 top=30 style="border:1px solid black;padding:10px 20px;"}

What is our 'best'<br>
estimate of the true<br>
effect in School A?

:::

## Further examples

* Item Response Theory modelling
  * Rasch model: $\mathrm{logit}[\mathrm{Pr}(y_{ij} = 1 | \theta_j)] = \theta_j - \beta_i$
  * 2-parameter model: $\mathrm{logit}[\mathrm{Pr}(y_{ij} = 1 | \theta_j)] = \alpha_i(\theta_j - \beta_i)$
* The `edstan` package provides functions and Stan code to fit IRT models

## Frequentist vs. Bayesian

* Bayesian analysis:
  * Uses prior information (when available)
  * Accounts for uncertainty in parameters
  * Provides probability values and credible parameters with more intuitive (and meaningful) interpretation
  * Can provide more reliable estimates in small samples
  * Can estimate more complicated models and conduct more informative post-estimation analyses than traditional approaches
* But: can be more difficult to implement in practice
  * This is not necessarily a bad thing -- it might make us think about some of the underlying assumptions that can be swept under the carpet in traditional statistical analysis

## Summary

* Conducting Bayesian analysis requires a little more thought, and a little more confidence with statistical methods, than 'traditional' frequentist approaches
* Modern software packages make this a lot easier (and are getting better all the time)
* The payoff is more flexible modelling options, ability to incorporate relevant prior information, more robust estimates in small samples, and more meaningful analysis of uncertainty in parameter estimates

## References

* Recommended textbooks:
  * Introductory: *Doing Bayesian Data Analysis: A tutorial with R, JAGS, and Stan*, 2^nd^ Edition. John K. Kruschke (2015)
    * Mostly non-technical presentation of applied Bayesian analysis
    * e-Book available for download via the library
  * Intermediate: *Statistical Rethinking: A Bayesian Course with Examples in R and Stan*, 2^nd^ Edition. Richard McElreath (2020)
    * How to think about statistics in terms of data generating models
    * I couldn't find an e-Book version, but there are video lectures by the author on YouTube: <https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus>
  * Advanced: *Bayesian Data Analysis*, 3^rd^ Edition. Andrew Gelman, John B. Carlin, Hal S. Stearn, David B. Dunson, Aki Vehtari, and Donald B. Rubin (2013)
    * Parts of this are quite advanced, but it also has lots of useful examples and applications
    * PDF version available from Gelman's website: <http://www.stat.columbia.edu/~gelman/book/>

## References

* Other useful resources:
  * Much of the background theory here follows *Bayesian Basics*, Michael Clark (2018): <https://m-clark.github.io/bayesian-basics/>
  * Stan and `rstan` both have extensive documentation, examples, etc. -- See <https://mc-stan.org>
  * Other R packages (`rstanarm`, `brms`, `edstan`, etc.) have their own documentation and usually good vignettes, tutorials, and/or examples
