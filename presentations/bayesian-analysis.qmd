---
title: "Introduction to Bayesian Analysis"
subtitle: "CMOR Lunch'n'Learn"
author: "Ross Wilson"
date: "2024-04-11"
date-format: "D MMMM YYYY"
format:
  revealjs:
    theme: [default, cmor.scss]
    title-slide-attributes: 
      data-background-image: "images/otago-pennant.png, images/cmor-banner.png, images/title-background.png"
      data-background-size: "7%, 25%, 100%"
      data-background-position: "3% 1%, bottom 1% left 50px,0 0"
      data-background-repeat: "no-repeat"
    template-partials:
      - styles.html
      - title-slide.html
    margin: 0.2
    footer: "[Return to CMOR Lunch'n'Learn presentations](../index.html)"
---
```{r setup}
library(tidyverse)
library(kableExtra, exclude = "group_rows")
```

## What is the objective of a statistical analysis?

* Consider, for example, a clinical trial to estimate a treatment effect

* We may want to know:

  * What is the 'true' (average) treatment effect?
  
  * How certain are we about that estimate?
  
    * What is the range of plausible values of the effect?
    
    * What is the probability that the treatment is 'effective'?

# Classical versus Bayesian approaches

## Classical versus Bayesian approaches

<br>

* We'll try not to go into too much technical detail

<br>

. . .

* A useful way I have seen it described:

  * Classical methods tell us what the observed data (e.g. from this trial) tell us about the treatment effect
  
  * Bayesian methods tell us how we should update our beliefs about the treatment effect based on these data

## Brief review of classical statistics

* Maximum likelihood - what true value of the treatment effect is most compatible with observed data?
  
  * *For what value of the treatment effect are the observed data most likely to occur?*

. . .

* Hypothesis testing - how likely is it that the observed data could be due to chance alone?

  * *If there is no true effect of treatment, how likely would the observed (or larger) effect be?*

. . .

* Confidence intervals - what range of values can we be 'confident' the treatment effect falls within?
  
  * Almost, but not quite: *There is a (say) 95% probability that the true effect lies within the 95% confidence interval*

## What is Bayesian analysis?

<br>

* A **probabilistic** approach to data analysis and interpretation

<br>

* Conceptually, we consider the treatment effect to be a **random variable**

  * We can express our beliefs about the value of this variable as a probability distribution

<br>

. . .

* How should we refine our prior beliefs in light of new data?

## Conditional probability

<br>

* Probability is conditional on available information

::: {.incremental}

* What is the probability it will rain in Dunedin tomorrow?

  * $\mathrm{Pr}(\text{Rain tomorrow})$

  * $\mathrm{Pr}(\text{Rain tomorrow}\ |\ \text{Today's weather})$

  * $\mathrm{Pr}(\text{Rain tomorrow}\ |\ \text{Tomorrow's forecast})$

<br>

* Even the first (unconditional) probability is implicitly conditional on some information set (e.g. your general knowledge about Dunedin's rainfall patterns)

:::

## What is Bayesian analysis?

<br>

* Bayesian analysis is the process of updating these probabilities in light of new information

. . .

* That is, given new $\mathit{data}$, how should we get from

$$\mathrm{Pr}(A)$$

. . .

::: {style="text-align:center"}
to
:::

$$\mathrm{Pr}(A\ |\ \mathit{data})$$

<br>

## Bayes' Theorem:

<br>

$$\mathrm{Pr}(A | B) = \frac{\mathrm{Pr}(B | A) \mathrm{Pr}(A)}{\mathrm{Pr}(B)}$$

<br>

. . .

  * In a statistical model, we are usually interested in probability distributions over continuous variables:

$$f(\theta | y) = \frac{f(y | \theta) f(\theta)}{f(y)}$$

  (updated knowledge about a parameter $\theta$ given data $y$)

## Bayes' Theorem

<br>

* Tells us how to calculate the 'posterior distribution' given:
  * an assumed data generating model
  * prior information/beliefs about the parameter(s)
  * current data

. . .

$$f(\theta | y) = \frac{f(y | \theta) f(\theta)}{f(y)}$$

. . .

$$\mathrm{Posterior} \propto \mathrm{Likelihood} \times \mathrm{Prior}$$

. . .

* Informally: updated belief = current evidence $\times$ prior evidence or belief

## Bayes' Theorem

* In principle:

  * we start with some beliefs about the parameters of interest
  
  * review those beliefs in light of the evidence at hand
  
  * and calculate an updated belief as a combination of the prior and the new evidence
  
```{r}
sigma <- 10
mu_0 <- 175; tau_0 <- 3
n <- 10; y_bar <- 178
density_tbl <- tibble(
    mu = seq(160, 190, length.out = 99999),
    prior = dnorm(mu, mu_0, tau_0),
    likelihood = dnorm(mu, y_bar, sigma/sqrt(n)),
    posterior = dnorm(mu, (mu_0 / tau_0^2 + y_bar / (sigma^2 / n)) / (1 / tau_0^2 + n / sigma^2), sqrt(1 / (1 / tau_0^2 + n / sigma^2)))
)
ymax <- max(density_tbl$prior, density_tbl$likelihood, density_tbl$posterior)
```

```{r}
p1 <- ggplot(density_tbl, aes(mu)) +
    geom_area(aes(y = prior), linetype = 0, fill = "#8888FF66", stat = "identity", position = "identity") +
    scale_x_continuous(limits = c(160, 190)) +
    scale_y_continuous("Density", limits = c(0, ymax), expand = c(0, 0)) +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.line.x = element_blank(), panel.grid = element_blank(),
          panel.background = element_blank(), plot.background = element_blank())
p2 <- p1 +
  geom_area(aes(y = likelihood), linetype = 0, fill = "#FF888866", stat = "identity", position = "identity")
p3 <- p2 +
  geom_area(aes(y = posterior), linetype = 0, fill = "#EE88CC66", stat = "identity", position = "identity")
ggsave("images/density-1.png", p1, width = 6, height = 4)
ggsave("images/density-2.png", p2, width = 6, height = 4)
ggsave("images/density-3.png", p3, width = 6, height = 4)
```

![](images/density-1.png){.absolute bottom=-60 left=230 width=675 .fragment .fade-in-then-out fragment-index=1}

![](images/density-2.png){.absolute bottom=-60 left=230 width=675 .fragment .fade-in-then-out fragment-index=2}

![](images/density-3.png){.absolute bottom=-60 left=230 width=675 .fragment fragment-index=3}

::: {.absolute bottom=180 left=430 .fragment fragment-index=1}
[Prior]{style="color:#9999FF;"}
:::


::: {.absolute bottom=130 left=750 .fragment fragment-index=2}
[Likelihood]{style="color:#FF9999;"}
:::


::: {.absolute bottom=300 left=650 .fragment fragment-index=3}
[Posterior]{style="color:#C76F8E;"}
:::
## Specifying priors

* The role of prior beliefs has been the most controversial aspect of Bayesian analysis

::: {.fragment fragment-index=1}

* Where do our 'priors' come from?

  * Previous research
  
  * Common sense/intuition?
  
  * 'Weakly informative' priors

:::
::: {.fragment fragment-index=2}

* In most cases there is no 'correct' prior, and sensitivity to various plausible priors should be considered

:::
::: {.fragment fragment-index=3}

* The more data we have,  
the less the prior matters

:::

```{r}
n2 <- 100
density_tbl2 <- density_tbl %>% 
  mutate(
  likelihood2 = dnorm(mu, y_bar, sigma/sqrt(n2)),
  posterior2 = dnorm(mu, (mu_0 / tau_0^2 + y_bar / (sigma^2 / n2)) / (1 / tau_0^2 + n2 / sigma^2), sqrt(1 / (1 / tau_0^2 + n2 / sigma^2)))
)

ymax2 <- max(density_tbl2$prior, density_tbl2$likelihood2, density_tbl2$posterior2)

p4 <- ggplot(density_tbl2, aes(mu)) +
    geom_area(aes(y = prior), linetype = 0, fill = "#8888FF66", stat = "identity", position = "identity") +
    scale_x_continuous(limits = c(160, 190)) +
    scale_y_continuous(NULL, limits = c(0, ymax2), expand = c(0, 0)) +
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(),
          axis.line.x = element_blank(), panel.grid = element_blank(),
          panel.background = element_blank(), plot.background = element_blank())
p5 <- p4 +
  geom_area(aes(y = likelihood2), linetype = 0, fill = "#FF888866", stat = "identity", position = "identity")
p6 <- p5 +
  geom_area(aes(y = posterior2), linetype = 0, fill = "#EE88CC66", stat = "identity", position = "identity")
ggsave("images/more-data-1.png", p4, width = 6, height = 10)
ggsave("images/more-data-2.png", p5, width = 6, height = 10)
ggsave("images/more-data-3.png", p6, width = 6, height = 10)
```

![](images/more-data-1.png){.absolute bottom=-60 left=210 width=800 height=1500 .fragment .fade-in-then-out fragment-index=4}

![](images/more-data-2.png){.absolute bottom=-60 left=210 width=800 height=1500 .fragment .fade-in-then-out fragment-index=5}

![](images/more-data-3.png){.absolute bottom=-60 left=210 width=800 height=1500 .fragment fragment-index=6}

## Why Bayesian analysis?

* Our objective is usually to conclude something about the likely values of a treatment effect (or other outcome of interest)

. . .

* There is usually at least some prior evidence relevant to the research question

  * Be explicit and transparent about the use of external evidence, judgements, and assumptions

. . .

* Can be much more flexible than traditional approaches

  * Using more---and more varied---sources of data
  
  * More flexible models tailored to particular situations

. . .

* Interpretation of results is much more intuitive (and relevant) than most traditional statistical methods

## Presentation and interpretation {.smaller}

* Bayesian analyses are often quite complex, and care is needed in presentation and interpretation

* The prior(s) used should always be explicitly stated and justified
  * Sensitivity to different plausible priors should be considered

* Bayesian analysis produces an estimated parameter *distribution*, not a single point estimate
  * The mean or median can be used as a central estimate
  * Quantiles of the distribution can be used to generate credible intervals
  * The probability of specific ranges of parameter values (e.g. $\mathrm{Pr}(\theta > 0)$) might be of interest

* It is important to note (as in classical analyses) that the distribution captures *uncertainty* in the parameter estimate, not between-person variability in treatment effects

## Comparing classical and Bayesian approaches {.smaller}

* Maximum likelihood
  * Classical approaches use the likelihood (i.e. observed data) alone
  * Bayesian approaches combine the observed data likelihood with prior beliefs

. . .

* Hypothesis testing
  * The classical p-value is ***not*** $\mathrm{Pr}(\text{no treatment effect})$
  * Bayesian analysis ***can*** tell us $\mathrm{Pr}(\text{no treatment effect})$ (or any other probability statement about the parameters of interest)

. . .

* Confidence intervals
  * Again, the classical confidence interval is ***not*** strictly a probability statement
  * In some (many) cases, classical confidence intervals are identical or very similar to Bayesian equivalents assuming no useful prior information
  * When prior information is available, Bayesian credible intervals incorporate that information

# Examples

## Examples

<br>

* These examples are taken from *Bayesian Approaches to Clinical Trials and Health-Care Evaluation* by Spiegelhalter et al. (2003), and *Bayesian Data Analysis* (Third edition) by Gelman et al. (2020)

<br>

* I hope to give some idea of the range and flexibility of Bayesian analysis

<br>

* We will focus on the 'What' and the 'Why', not on the 'How'

## Diagnostic testing

. . .

<br>

* Suppose have a new HIV test, which has 95% sensitivity and 98% specificity

* We want to use the test for screening, in a population with HIV prevalence of 1/1000

<br>

. . .

* What is the probability that someone has HIV, given that they have tested positive?

## Diagnostic testing

* Consider the expected status of 100,000 individuals being screened

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> </th>
   <th style="text-align:center;"> <b>HIV<sup>&minus;</sup></b> </th>
   <th style="text-align:center;"> <b>HIV<sup>+</sup></b> </th>
   <th style="text-align:center;"> </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> <b>Test<sup>&minus;</sup></b> </td>
   <td style="text-align:center;"> 97 902 </td>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 97 903 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> <b>Test<sup>+</sup></b> </td>
   <td style="text-align:center;"> 1 998 </td>
   <td style="text-align:center;"> 95 </td>
   <td style="text-align:center;"> 2 093 </td>
  </tr>
  <tr>
   <td style="text-align:center;">  </td>
   <td style="text-align:center;"> 99 900 </td>
   <td style="text-align:center;"> 100 </td>
   <td style="text-align:center;"> 100 000 </td>
  </tr>
</tbody>
</table>
```

. . .

* The probability of having HIV given a positive test can be calculated:
$$\mathrm{Pr}(\text{HIV}^+\ |\ \text{test}^+) = \frac{95}{2093} = 0.045$$

## Diagnostic testing

* Using Bayes' theorem:
$$\mathrm{Pr}(\text{HIV}^+\ |\ \text{test}^+) = \frac{\mathrm{Pr}(\text{test}^+\ |\ \text{HIV}^+)\ \mathrm{Pr}(\text{HIV}^+)}{\mathrm{Pr}(\text{test}^+)}$$

  * $\mathrm{Pr}(\text{test}^+\ |\ \text{HIV}^+)$ is the sensitivity, $0.95$
  
  * $\mathrm{Pr}(\text{HIV}^+)$ is $1 / 1000 = 0.001$
  
  * $\mathrm{Pr}(\text{test}^+)$ can be calculated as  
  $\mathrm{Pr}(\text{test}^+\ |\ \text{HIV}^-)\ \mathrm{Pr}(\text{HIV}^-) + \mathrm{Pr}(\text{test}^+\ |\ \text{HIV}^+)\ \mathrm{Pr}(\text{HIV}^+)$  
  $\ \ \ \  = 0.02 \times 0.999 + 0.95 \times 0.001 = 0.02093$

* Therefore,
$$\mathrm{Pr}(\text{HIV}^+\ |\ \text{test}^+) = \frac{0.95 \times 0.001}{0.02093} = 0.045$$

## Diagnostic testing

* More than 95% of those testing positive will not actually have HIV!

<br>

. . .

* Note that this is very sensitive to the base rate (the proportion who have HIV in the population being tested)

* If the test was used for diagnosis in people suspected of having HIV, such that, say $\mathrm{Pr}(\text{HIV}^+) = 0.2$, then
$$\mathrm{Pr}(\text{HIV}^+\ |\ \text{test}^+) = \frac{0.95 \times 0.2}{0.206} = 0.922$$

## A simple RCT analysis

. . .

* The GREAT trial was a study of a new drug for early treatment after myocardial infarction, compared with placebo

* The primary outcome was 30-day mortality rate, with data:

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> </th>
   <th style="text-align:center;"> <b>New</b> </th>
   <th style="text-align:center;"> <b>Placebo</b> </th>
   <th style="text-align:center;"> </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> <b>Death</b> </td>
   <td style="text-align:center;"> 13 </td>
   <td style="text-align:center;"> 23 </td>
   <td style="text-align:center;"> 36 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> <b>No death</b> </td>
   <td style="text-align:center;"> 150 </td>
   <td style="text-align:center;"> 125 </td>
   <td style="text-align:center;"> 275 </td>
  </tr>
  <tr>
   <td style="text-align:center;">  </td>
   <td style="text-align:center;"> 163 </td>
   <td style="text-align:center;"> 148 </td>
   <td style="text-align:center;"> 311 </td>
  </tr>
</tbody>
</table>
```

## A simple RCT analysis {.smaller}

* Standard analysis of these data gives an OR of (13 / 150) / (23 / 125) = 0.47, with 95% CI 0.24 to 0.97

  * That is, a fairly large reduction in mortality, just reaching statistical significance at the 5% level

. . . 

* From a Bayesian perspective, we need to consider what our prior belief is (was) on the plausible range of true effects, and how these results should cause us to revise those beliefs

. . . 

* Prior distribution was based on the subjective judgement of a senior cardiologist, informed by previous published and unpublished studies

  * 'an expectation of 15--20% reduction in mortality is highly plausible, while the extremes of no benefit and a 40% relative reduction are both unlikely'
  
  * Two initial thoughts on this prior:
    
    * This is a strong prior judgement, compared to the amount of information provided by the trial
  
    * If we are already confident that the treatment is effective, why are we doing the trial?

. . . 

* Combining this prior with the observed data gives posterior estimate of 0.73 (95% credible interval 0.58 to 0.93)

## A simple RCT analysis

* Bayesian analysis can also be used to ask how the results of this trial should change the views of a reasonably skeptical observer

  * That is, with no prior view one way or the other, but believing large treatment effects are unlikely

* Assuming a prior centred on no effect (OR = 1), with 95% interval from 50% reduction (OR = 0.5) to 100% increase (OR = 2):

* Posterior OR = 0.70 (95% interval 0.43 to 1.14), i.e., no effect would still be considered reasonably plausible

## Multiplicity and hierarchical models

. . .

* Problems of multiplicity are well-recognised in traditional statistical analyses
  * If we test multiple outcomes, multiple subsets of participants, multiple treatment group contrasts, we will inevitably find some 'significant' results, even if there are no true effects

* In a Bayesian analysis, these multiple endpoints can often be nested within a larger hierarchical model

## Educational coaching interventions in 8 schools {.smaller}

* A study was performed of coaching programs to improve SAT scores in each of 8 schools
  * Estimates of the treatment effect were obtained separately from each school

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> School </th>
   <th style="text-align:center;"> Estimated treatment effect </th>
   <th style="text-align:center;"> Standard error of effect estimate </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> A </td>
   <td style="text-align:center;"> 28 </td>
   <td style="text-align:center;"> 15 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> B </td>
   <td style="text-align:center;"> 8 </td>
   <td style="text-align:center;"> 10 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> C </td>
   <td style="text-align:center;"> &minus;3 </td>
   <td style="text-align:center;"> 16 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> D </td>
   <td style="text-align:center;"> 7 </td>
   <td style="text-align:center;"> 11 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> E </td>
   <td style="text-align:center;"> &minus;1 </td>
   <td style="text-align:center;"> 9 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> F </td>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 11 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> G </td>
   <td style="text-align:center;"> 18 </td>
   <td style="text-align:center;"> 10 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> H </td>
   <td style="text-align:center;"> 12 </td>
   <td style="text-align:center;"> 18 </td>
  </tr>
</tbody>
</table>
```

## Educational coaching interventions in 8 schools

* We can distinguish 3 different assumptions about the relationship between these estimates:

  * *Identical parameters*: All the true effects are identical, and the observed differences are due to sampling variation
  
  * *Independent parameters*: The true effects are independent---knowledge about one tells us nothing about the likely values of the others
  
  * *Exchangeable parameters*: The true effects are different, but drawn from a common distribution
  
    * The results from each school will affect our estimate of that common distribution, and therefore our estimate of the 'true' effect in the other schools

## Educational coaching interventions in 8 schools {.smaller}

```{r}
eightschools <- tibble(
  school = factor(rep(LETTERS[1:8], 2), rev(LETTERS[1:8])),
  model = factor(rep(c("Independent", "Exchangeable"), each = 8), c("Independent", "Exchangeable")),
  effect = c(28, 8, -3, 7, -1, 1, 18, 12, 10, 8, 7, 8, 5, 6, 10, 8),
  se = c(15, 10, 16, 11, 9, 11, 10, 18, rep(NA, 8)),
  lower = if_else(!is.na(se), effect - qnorm(0.975) * se, 
                  c(rep(NA, 8), -2, -5, -11, -7, -9, -7, -1, -6)),
  upper = if_else(!is.na(se), effect + qnorm(0.975) * se, 
                  c(rep(NA, 8), 31, 23, 19, 21, 18, 28, 26, 33))
)
p_8schools <- eightschools %>% 
  ggplot(aes(school, effect, ymin = lower, ymax = upper, colour = model)) +
  geom_hline(yintercept = 0) +
  geom_linerange(position = position_dodge(width = -0.5), show.legend = FALSE) +
  geom_point(position = position_dodge(width = -0.5)) +
  scale_y_continuous("Treatment effect") +
  scale_x_discrete(NULL) +
  scale_color_brewer(NULL, type = "qual", palette = 6) +
  coord_flip() +
  theme(legend.position = "bottom")

ggsave("images/8-schools.png", p_8schools, width = 6, height = 6)
```


* What do these different assumptions imply for our results and interpretation?

  * *Identical parameters*: We can pool the results from all studies (weighted by the inverse of the sampling variances)
  
    * We get an estimate of 7.7 points (S.E. 4.1) for the (common) treatment effect in all schools
  
  * *Independent parameters*: Take the estimates in the table at face value
  
  * *Exchangeable parameters*: Estimate a Bayesian hierarchical model
  
    * Assume the 'true' effects in each school are drawn from a normal (or other) distribution, and estimate the parameters (mean, sd) of that distribution
    
    * This requires us to specify prior beliefs about the mean and standard deviation of the effect distribution  
(For now, we assume non-informative prior distributions for both)

    * The effects for all schools are pulled towards the sample mean (between 5 and 10 points, instead of between --3 and 28, but with substantial uncertainty)

![](images/8-schools.png){.absolute bottom=-60 left=210 width=900 height=900 .fragment}

## Meta-analysis {.smaller}

. . .

* Similar hierarchical models can be used for meta-analysis as well

* We consider an example of a meta-analysis of beta-blockers for reducing mortality after myocardial infarction

* The study included 22 clinical trials, with data for the first few as shown

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> Study </th>
   <th style="text-align:center;"> Control </th>
   <th style="text-align:center;"> Treated </th>
   <th style="text-align:center;"> Log(OR) </th>
   <th style="text-align:center;"> SE </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 3/39 </td>
   <td style="text-align:center;"> 3/38 </td>
   <td style="text-align:center;"> 0.028 </td>
   <td style="text-align:center;"> 0.850 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 14/116 </td>
   <td style="text-align:center;"> 7/114 </td>
   <td style="text-align:center;"> -0.741 </td>
   <td style="text-align:center;"> 0.483 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 11/93 </td>
   <td style="text-align:center;"> 5/69 </td>
   <td style="text-align:center;"> -0.541 </td>
   <td style="text-align:center;"> 0.565 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 127/1520 </td>
   <td style="text-align:center;"> 102/1533 </td>
   <td style="text-align:center;"> -0.246 </td>
   <td style="text-align:center;"> 0.138 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 27/365 </td>
   <td style="text-align:center;"> 28/355 </td>
   <td style="text-align:center;"> 0.069 </td>
   <td style="text-align:center;"> 0.281 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 6 </td>
   <td style="text-align:center;"> 6/52 </td>
   <td style="text-align:center;"> 4/59 </td>
   <td style="text-align:center;"> -0.584 </td>
   <td style="text-align:center;"> 0.676 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 7 </td>
   <td style="text-align:center;"> 152/939 </td>
   <td style="text-align:center;"> 98/945 </td>
   <td style="text-align:center;"> -0.512 </td>
   <td style="text-align:center;"> 0.139 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 8 </td>
   <td style="text-align:center;"> 48/471 </td>
   <td style="text-align:center;"> 60/632 </td>
   <td style="text-align:center;"> -0.079 </td>
   <td style="text-align:center;"> 0.204 </td>
  </tr>
</tbody>
</table>
```

## Meta-analysis

* As before, if we assume exchangeability between the studies, we can estimate a Bayesian hierarchical model for the treatment effects

* The results, using a non-informative prior, are

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> Estimand </th>
   <th style="text-align:center;"> 2.5% </th>
   <th style="text-align:center;"> 25% </th>
   <th style="text-align:center;"> Median </th>
   <th style="text-align:center;"> 75% </th>
   <th style="text-align:center;"> 97.5% </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> Mean </td>
   <td style="text-align:center;"> &minus;0.37 </td>
   <td style="text-align:center;"> &minus;0.29 </td>
   <td style="text-align:center;"> &minus;0.25 </td>
   <td style="text-align:center;"> &minus;0.20 </td>
   <td style="text-align:center;"> &minus;0.11 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Standard deviation </td>
   <td style="text-align:center;"> 0.02 </td>
   <td style="text-align:center;"> 0.08 </td>
   <td style="text-align:center;"> 0.13 </td>
   <td style="text-align:center;"> 0.18 </td>
   <td style="text-align:center;"> 0.31 </td>
  </tr>
</tbody>
</table>
```

## Meta-analysis

* There are several estimands that may be of interest:

  * The mean of the distribution of effect sizes
  
  * The effect size in any of the observed studies
  
  * The effect size in a new comparable study

```{=html}
<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:center;"> Estimand </th>
   <th style="text-align:center;"> 2.5% </th>
   <th style="text-align:center;"> 25% </th>
   <th style="text-align:center;"> Median </th>
   <th style="text-align:center;"> 75% </th>
   <th style="text-align:center;"> 97.5% </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> Mean </td>
   <td style="text-align:center;"> &minus;0.37 </td>
   <td style="text-align:center;"> &minus;0.29 </td>
   <td style="text-align:center;"> &minus;0.25 </td>
   <td style="text-align:center;"> &minus;0.20 </td>
   <td style="text-align:center;"> &minus;0.11 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Standard deviation </td>
   <td style="text-align:center;"> 0.02 </td>
   <td style="text-align:center;"> 0.08 </td>
   <td style="text-align:center;"> 0.13 </td>
   <td style="text-align:center;"> 0.18 </td>
   <td style="text-align:center;"> 0.31 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Predicted effect </td>
   <td style="text-align:center;"> &minus;0.58 </td>
   <td style="text-align:center;"> &minus;0.34 </td>
   <td style="text-align:center;"> &minus;0.25 </td>
   <td style="text-align:center;"> &minus;0.17 </td>
   <td style="text-align:center;"> 0.11 </td>
  </tr>
</tbody>
</table>
```

## One more example to be added

* Something demonstrating the use of external data (e.g. observational studies as well as trials)

# Bayesian analysis in practice

## Software

* Historically, conducting Bayesian analysis required specialised software/modelling languages
  * BUGS (Bayesian inference Using Gibbs Sampling)
  * JAGS (Just Another Gibbs Sampler)
  * Stan

. . .

* Most general-purpose statistical programs these days have (at least some) Bayesian methods available
  * R: e.g. `rstanarm`, `brms`
    * Also direct interface to Bayesian modelling languages with `rstan`, `rjags`
  * Stata (also StataStan)
  * SAS, SPSS ?

## Model checking

* We won't go into this in any detail today, but it is a very important part of any Bayesian analysis
* Posterior predictive checks
  * The model can be used to predict outcomes both in- and out-of-sample
  * Do these predictions make sense (face validity)? Are they consistent with observed data?
* Model convergence
  * Bayesian models are generally estimated by simulation methods
  * Has the simulation converged to a stable distibution of parameter values?

## References {.smaller}

* Recommended textbooks:
  * Introductory: *Doing Bayesian Data Analysis: A tutorial with R, JAGS, and Stan*, 2^nd^ Edition. John K. Kruschke (2015)
    * Mostly non-technical presentation of applied Bayesian analysis
    * eBook available for download via the library
  * Intermediate: *Statistical Rethinking: A Bayesian Course with Examples in R and Stan*, 2^nd^ Edition. Richard McElreath (2020)
    * How to think about statistics in terms of data generating models
    * I couldn't find an eBook version, but there are video lectures by the author on YouTube: <https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus>
  * Intermediate: *Bayesian Approaches to Clinical Trials and Health-Care Evaluation*. David J. Spiegelhalter, Keith R. Abrams, Jonathan P. Myles (2004)
    * Focus on RCTs and some other clinical research designs, presenting several interesting examples of different uses of Bayesian methods
    * eBook available for download via the library
  * Advanced: *Bayesian Data Analysis*, 3^rd^ Edition. Andrew Gelman, John B. Carlin, Hal S. Stearn, David B. Dunson, Aki Vehtari, and Donald B. Rubin (2013)
    * Parts of this are quite advanced, but it also has lots of useful examples and applications
    * PDF version available from Gelman's website: <http://www.stat.columbia.edu/~gelman/book/>
